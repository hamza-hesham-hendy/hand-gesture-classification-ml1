import mlflow
import mlflow.data
import mlflow.models
import mlflow.sklearn
from evaluation import evaluate_model
import logging
def setup_mlflow_experiment(experiment_name: str, tracking_uri: str = "http://localhost:5000") -> str:
    mlflow.set_tracking_uri(tracking_uri)
    exp = mlflow.set_experiment(experiment_name)
    return exp.experiment_id

def log_model_with_mlflow(model, X_test, y_test, model_name: str, exp_id: str):
    with mlflow.start_run(experiment_id=exp_id, run_name=model_name) as run:
        logging.info(f"Logging {model_name} to MLflow...")

        mlflow.set_tag("model", model_name)

        # Log ALL grid trials as nested runs
        for i in range(len(model.cv_results_['params'])):
            with mlflow.start_run(
                run_name=f"{model_name}_trial_{i}",
                nested=True
            ):
                mlflow.log_params(model.cv_results_['params'][i])
                mlflow.log_metric(
                    "mean_test_score",
                    model.cv_results_['mean_test_score'][i]
                )
                mlflow.log_metric(
                    "std_test_score",
                    model.cv_results_['std_test_score'][i]
                )

        # Log best params and metrics
        accuracy, f1, auc, pred = evaluate_model(model, X_test, y_test)

        mlflow.log_params(model.best_params_)
        mlflow.log_metrics({
            "Mean CV score": model.best_score_,
            "Accuracy": accuracy,
            "f1-score": f1,
            "AUC": auc
        })

        pd_dataset = mlflow.data.from_pandas(X_test, name="Testing Dataset")
        mlflow.log_input(pd_dataset, context="Testing")

        signature = mlflow.models.infer_signature(X_test, y_test)
        mlflow.sklearn.log_model(model, model_name, signature=signature, input_example=X_test.iloc[[0]])
